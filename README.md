# ğŸŒŒ Lakehouse_Alchemy: Bronze_to_Gold_Pipeline

![Databricks](https://img.shields.io/badge/Platform-Databricks-orange?logo=databricks)

![PySpark](https://img.shields.io/badge/PySpark-ETL-blue?logo=apachespark)

![Delta Lake](https://img.shields.io/badge/Delta%20Lake-Storage-brightgreen)

![Spark SQL](https://img.shields.io/badge/Spark%20SQL-Analytics-purple?logo=apachespark)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

> **From raw JSON chaos to curated analytics â€” an end-to-end PySpark Lakehouse pipeline.**

---

## ğŸš€ Project Overview

**Lakehouse-Alchemy** is a robust and scalable data engineering project showcasing an **end-to-end ETL pipeline** built on the **Medallion Architecture** within a **Data Lakehouse**.

It leverages **PySpark Structured Streaming** to process unstructured JSON data through dynamic **Bronze, Silver, and Gold layers**, transforming raw data into analytics-ready tables for business insights.

ğŸ›  Hosted on **Databricks**, this pipeline demonstrates modern data engineering practices with a focus on:

* Scalability âš¡
* Data Quality âœ…
* Performance Optimization ğŸš€

---

## âœ¨ Key Features

* ğŸŸ¤ **Dynamic Bronze Layer**: Ingests raw JSON with inferred schemas, capturing metadata for flexible handling of unstructured data.
* âšª **Silver Layer**: Flattens nested structures, deduplicates records, and ensures data quality.
* ğŸŸ¡ **Gold Layer**: Creates analytics-ready **dimension** and **fact tables** optimized for reporting.
* ğŸ—ï¸ **Medallion Architecture**: Organizes data into **Bronze (raw)**, **Silver (cleaned)**, **Gold (curated)**.
* ğŸ§¹ **Data Quality Framework** â€“ Validates primary keys, strings, dates, numeric ranges, and entity-specific rules.
* ğŸ”„ **Structured Streaming**: Supports batch and continuous processing with fault tolerance.
* ğŸ’¾ **Delta Lake**: Provides ACID transactions, schema enforcement, and time travel.
* ğŸ“ˆ **Optimized Analytics** â€“ ZORDER indexing on fact tables for faster queries.
  

---

## ğŸ—ï¸ Architecture

The pipeline follows the **Medallion Architecture**:

![Architecture Diagram](data_lakehouse/diagrams/lakehouse_alchemy_project_architecture.png)

### ğŸ¥‰Bronze Layer (`bronze_ingestion.py`)

* Ingests raw JSON from `/Volumes/pyspark_dwh/source/source_data`.
* Wraps data into `payload` struct with **file path + ingestion timestamp**.
* Streams into `pyspark_dwh.bronze` Delta tables.

### ğŸ¥ˆSilver Layer (`silver_transformation.py`)

* Flattens nested structs and arrays.
* Deduplicates by **entity-specific primary keys**.
* Adds `silver_ingest_ts`.
* Streams into `pyspark_dwh.silver`.

### ğŸ¥‡Gold Layer (`golden_transformation.py`)

* Builds **star schema**:

  * `dim_customers` ğŸ§‘â€ğŸ¤â€ğŸ§‘
  * `dim_products` ğŸ“¦
  * `fact_sales` ğŸ’°
* Validates data (non-null IDs, positive quantities/prices).
* Optimized with **ZORDER**.

---

ğŸ”„ Pipeline Automation ğŸš€

This project includes automated workflows in Databricks to orchestrate the Medallion Architecture ETL pipeline and data quality checks.

It supports both batch and streaming ingestion, ensuring scalability, reliability, and production-grade execution from raw data (Bronze) to business-ready insights (Gold).

![Databricks Workflow Automation](./data_lakehouse/diagrams/lakehouse_alchemy_workflow.png)


---

## âš™ï¸ Technologies Used

* ğŸ **Python**, **PySpark**, **SparkSQL**
* ğŸ”„ **PySpark Structured Streaming**
* ğŸ’¾ **Delta Lake**
* â˜ï¸ **Databricks**
* ğŸ“‚ **Git/GitHub**
* ğŸ“ **Jupyter Notebooks**
* ğŸ¨ **Draw\.io** for diagrams

---

## ğŸ“‚ Project Structure

```
ğŸ“‚ **data_lakehouse**/

â”£ ğŸ“‚ **datasets**/ â†’  JSON files (unstructured data)

â”£ ğŸ“‚ **diagrams**/ â†’ data architecture & schema documentations

â”£ ğŸ“‚ **etl_scripts**/ â†’ ETL code (`bronze_layer.py`, `silver_transformation.py`, `gold_transformation.py`)

â”£ ğŸ“‚ **data_quality_checks**/ â†’ Data quality & pipeline validation
```

---

## ğŸ“Š Use Cases

* ğŸ“ˆ **Real-Time Analytics**: Dashboards in Power BI/Tableau.
* ğŸ¢ **Data Warehousing**: Build scalable Lakehouse.
* âœ… **Data Quality**: Deduplication & validation.
* âš¡ **Scalability**: Handle massive JSON datasets.

---

## ğŸ¯ Future Enhancements

* â±ï¸ Continuous streaming triggers (`processingTime = '1 minute'`).
* ğŸ“Š Direct integration with Tableau/Power BI.
* ğŸ”” Monitoring & alerting for pipeline failures.
* âœ… Data observability with **Great Expectations**.
* âš™ï¸ CI/CD automation using **GitHub Actions**.

---

## ğŸ“š Learn More

* [Delta Lake Documentation](https://docs.delta.io)
* [PySpark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
* [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)

---

## ğŸ“¬ Contact

* **Author**: Sergi Tkeshelashvili
* ğŸ“‚ **Repository**: [Lakehouse-Alchemy-Bronze-to-Gold-Pipeline](https://github.com/sergitkeshelashvili/Lakehouse-Alchemy-Bronze-to-Gold-Pipeline)
* ğŸ’¼ LinkedIn: [https://www.linkedin.com/in/sergi-tkeshelashvili-022011383](https://www.linkedin.com/in/sergi-tkeshelashvili-022011383)

---

## ğŸ›¡ï¸ License  

Licensed under the **MIT License**.  

---

âœ¨ *Turn raw data into business gold with Lakehouse-Alchemy!* âœ¨

